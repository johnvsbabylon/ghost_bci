"""
Professional-Grade Modular & Scalable Ghost Bot BCI Trainer
- Distributed training (DDP, FSDP)
- Mixed precision (AMP)
- Gradient accumulation & clipping
- Advanced checkpointing
- Multiple logging backends
- Modular dataset pipeline
- Curriculum learning
- Production-ready
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.cuda.amp import autocast, GradScaler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import numpy as np
from tqdm import tqdm
import wandb

# Import the model (assuming it's in the same directory)
# from ghost_bot_bci import GhostBotBCI


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class TrainingConfig:
    """Complete training configuration"""
    
    # Model
    vocab_size: int = 10000
    embed_dim: int = 256
    num_layers: int = 6
    num_affect: int = 8
    num_heads: int = 8
    mem_size: int = 150
    stream_len: int = 24
    n_mels: int = 80
    img_size: int = 224
    num_joints: int = 24
    bci_channels: int = 64
    
    # Training
    batch_size: int = 8
    num_epochs: int = 100
    learning_rate: float = 3e-4
    weight_decay: float = 0.01
    warmup_steps: int = 1000
    max_grad_norm: float = 1.0
    gradient_accumulation_steps: int = 1
    
    # Optimization
    optimizer: str = "adamw"  # adamw, adam, sgd, lion
    scheduler: str = "cosine"  # cosine, linear, constant
    betas: Tuple[float, float] = (0.9, 0.999)
    eps: float = 1e-8
    
    # Mixed precision
    use_amp: bool = True
    amp_dtype: str = "float16"  # float16 or bfloat16
    
    # Distributed
    use_ddp: bool = False
    use_fsdp: bool = False
    world_size: int = 1
    local_rank: int = 0
    
    # Data
    data_path: str = "./data"
    num_workers: int = 4
    pin_memory: bool = True
    prefetch_factor: int = 2
    
    # Checkpointing
    checkpoint_dir: str = "./checkpoints"
    save_every: int = 1000
    keep_last_n: int = 5
    resume_from: Optional[str] = None
    
    # Logging
    log_every: int = 10
    eval_every: int = 500
    use_wandb: bool = True
    use_tensorboard: bool = False
    project_name: str = "ghostbot-bci"
    run_name: Optional[str] = None
    
    # Loss weights
    loss_weights: Dict[str, float] = field(default_factory=lambda: {
        'language': 1.0,
        'collision': 0.5,
        'coherence': 0.3,
        'emotion': 0.2,
        'reconstruction': 0.1
    })
    
    # Curriculum learning
    use_curriculum: bool = False
    curriculum_schedule: List[Dict[str, Any]] = field(default_factory=list)
    
    # Hardware
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    
    @classmethod
    def from_yaml(cls, path: str):
        with open(path, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)
    
    def save(self, path: str):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            yaml.dump(self.__dict__, f)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATASET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GhostBotDataset(Dataset):
    """Modular dataset for Ghost Bot BCI training"""
    
    def __init__(
        self,
        data_path: str,
        split: str = "train",
        sequence_length: int = 30,
        include_modalities: List[str] = None
    ):
        self.data_path = Path(data_path)
        self.split = split
        self.sequence_length = sequence_length
        
        # Available modalities
        self.available_modalities = ['visual', 'audio', 'language', 'touch', 
                                     'proprio', 'vestib', 'bci']
        self.include_modalities = include_modalities or self.available_modalities
        
        # Load manifest
        manifest_path = self.data_path / f"{split}_manifest.json"
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                self.manifest = json.load(f)
        else:
            self.manifest = self._scan_directory()
            self._save_manifest()
        
        logging.info(f"Loaded {len(self.manifest)} samples for {split} split")
    
    def _scan_directory(self) -> List[Dict[str, str]]:
        """Scan directory for all available data"""
        manifest = []
        split_dir = self.data_path / self.split
        
        if not split_dir.exists():
            logging.warning(f"Directory {split_dir} not found. Creating empty manifest.")
            return manifest
        
        # Scan for sequences
        for seq_dir in sorted(split_dir.glob("seq_*")):
            entry = {'id': seq_dir.name}
            
            # Check for each modality
            for mod in self.available_modalities:
                mod_file = seq_dir / f"{mod}.npy"
                if mod_file.exists():
                    entry[mod] = str(mod_file)
            
            # Check for labels
            label_file = seq_dir / "labels.npy"
            if label_file.exists():
                entry['labels'] = str(label_file)
            
            manifest.append(entry)
        
        return manifest
    
    def _save_manifest(self):
        """Save manifest to disk"""
        manifest_path = self.data_path / f"{self.split}_manifest.json"
        os.makedirs(self.data_path, exist_ok=True)
        with open(manifest_path, 'w') as f:
            json.dump(self.manifest, f, indent=2)
    
    def __len__(self) -> int:
        return len(self.manifest)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Load a single sample"""
        entry = self.manifest[idx]
        sample = {}
        
        # Load each requested modality
        for mod in self.include_modalities:
            if mod in entry:
                data = np.load(entry[mod])
                sample[mod] = torch.from_numpy(data).float()
            else:
                # Return zeros if modality missing
                sample[mod] = self._get_zero_modality(mod)
        
        # Load labels if available
        if 'labels' in entry:
            labels = np.load(entry['labels'])
            sample['labels'] = torch.from_numpy(labels).long()
        
        return sample
    
    def _get_zero_modality(self, modality: str) -> torch.Tensor:
        """Generate zero tensor for missing modality"""
        shapes = {
            'visual': (self.sequence_length, 3, 224, 224),
            'audio': (self.sequence_length, 80),
            'language': (self.sequence_length,),
            'touch': (self.sequence_length, 1, 32, 32),
            'proprio': (self.sequence_length, 24),
            'vestib': (self.sequence_length, 6),
            'bci': (self.sequence_length, 64, 250)
        }
        return torch.zeros(shapes.get(modality, (self.sequence_length,)))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOSS FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GhostBotLoss(nn.Module):
    """Modular multi-task loss for Ghost Bot"""
    
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.config = config
        self.weights = config.loss_weights
        
        # Individual loss components
        self.lang_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)
        self.mse_loss_fn = nn.MSELoss()
        self.coherence_loss_fn = nn.MSELoss()
    
    def forward(self, outputs: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """Compute total loss and individual components"""
        
        losses = {}
        total_loss = 0.0
        
        # Language modeling loss
        if 'language_logits' in outputs and 'labels' in targets:
            B, T, V = outputs['language_logits'].shape
            logits = outputs['language_logits'].reshape(-1, V)
            labels = targets['labels'].reshape(-1)
            lang_loss = self.lang_loss_fn(logits, labels)
            losses['language'] = lang_loss.item()
            total_loss += self.weights['language'] * lang_loss
        
        # Collision data consistency (minimize variance in collision representation)
        if 'collision_data' in outputs:
            collision_loss = outputs['collision_data'].var(dim=1).mean()
            losses['collision'] = collision_loss.item()
            total_loss += self.weights['collision'] * collision_loss
        
        # Coherence loss (maximize human-AI synchronization)
        if 'coherence' in outputs:
            target_coherence = torch.ones_like(outputs['coherence'])
            coherence_loss = self.coherence_loss_fn(outputs['coherence'], target_coherence)
            losses['coherence'] = coherence_loss.item()
            total_loss += self.weights['coherence'] * coherence_loss
        
        # Emotion stability (prevent wild emotional swings)
        if 'emotion' in outputs:
            # Penalize large changes in emotion
            emo_loss = outputs['emotion'].abs().mean()
            losses['emotion'] = emo_loss.item()
            total_loss += self.weights['emotion'] * emo_loss
        
        # Reconstruction loss (if doing autoencoding)
        if 'reconstruction' in outputs and 'reconstruction_target' in targets:
            recon_loss = self.mse_loss_fn(outputs['reconstruction'], 
                                         targets['reconstruction_target'])
            losses['reconstruction'] = recon_loss.item()
            total_loss += self.weights['reconstruction'] * recon_loss
        
        losses['total'] = total_loss.item()
        return total_loss, losses


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZER & SCHEDULER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_optimizer(model: nn.Module, config: TrainingConfig) -> torch.optim.Optimizer:
    """Get optimizer based on config"""
    
    # Separate parameters for weight decay
    decay_params = []
    no_decay_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        
        # Don't apply weight decay to biases, LayerNorm, embeddings
        if 'bias' in name or 'norm' in name or 'embedding' in name:
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    param_groups = [
        {'params': decay_params, 'weight_decay': config.weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ]
    
    if config.optimizer.lower() == 'adamw':
        return torch.optim.AdamW(
            param_groups,
            lr=config.learning_rate,
            betas=config.betas,
            eps=config.eps
        )
    elif config.optimizer.lower() == 'adam':
        return torch.optim.Adam(
            param_groups,
            lr=config.learning_rate,
            betas=config.betas,
            eps=config.eps
        )
    elif config.optimizer.lower() == 'sgd':
        return torch.optim.SGD(
            param_groups,
            lr=config.learning_rate,
            momentum=0.9
        )
    else:
        raise ValueError(f"Unknown optimizer: {config.optimizer}")


def get_scheduler(optimizer: torch.optim.Optimizer, 
                 config: TrainingConfig,
                 num_training_steps: int):
    """Get learning rate scheduler"""
    
    if config.scheduler.lower() == 'cosine':
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=num_training_steps - config.warmup_steps,
            eta_min=config.learning_rate * 0.1
        )
    elif config.scheduler.lower() == 'linear':
        return torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=1.0,
            end_factor=0.1,
            total_iters=num_training_steps - config.warmup_steps
        )
    elif config.scheduler.lower() == 'constant':
        return torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)
    else:
        raise ValueError(f"Unknown scheduler: {config.scheduler}")


class WarmupScheduler:
    """Warmup wrapper for any scheduler"""
    
    def __init__(self, optimizer, warmup_steps: int, base_scheduler=None):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.base_scheduler = base_scheduler
        self.current_step = 0
        self.base_lr = optimizer.param_groups[0]['lr']
    
    def step(self):
        self.current_step += 1
        
        if self.current_step <= self.warmup_steps:
            # Linear warmup
            lr = self.base_lr * (self.current_step / self.warmup_steps)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
        elif self.base_scheduler is not None:
            self.base_scheduler.step()
    
    def get_last_lr(self):
        if self.current_step <= self.warmup_steps:
            return [self.base_lr * (self.current_step / self.warmup_steps)]
        elif self.base_scheduler is not None:
            return self.base_scheduler.get_last_lr()
        return [self.base_lr]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GhostBotTrainer:
    """Professional-grade trainer for Ghost Bot BCI"""
    
    def __init__(self, model: nn.Module, config: TrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # Setup distributed training
        self.is_distributed = config.use_ddp or config.use_fsdp
        if self.is_distributed:
            self._setup_distributed()
        
        # Model
        self.model = model.to(self.device)
        if config.use_ddp and self.is_distributed:
            self.model = DDP(self.model, device_ids=[config.local_rank])
        
        # Loss
        self.criterion = GhostBotLoss(config)
        
        # Datasets
        self.train_dataset = GhostBotDataset(config.data_path, split="train")
        self.val_dataset = GhostBotDataset(config.data_path, split="val")
        
        # Data loaders
        self.train_loader = self._get_dataloader(self.train_dataset, shuffle=True)
        self.val_loader = self._get_dataloader(self.val_dataset, shuffle=False)
        
        # Optimizer & scheduler
        num_training_steps = len(self.train_loader) * config.num_epochs
        self.optimizer = get_optimizer(self.model, config)
        base_scheduler = get_scheduler(self.optimizer, config, num_training_steps)
        self.scheduler = WarmupScheduler(self.optimizer, config.warmup_steps, base_scheduler)
        
        # Mixed precision
        self.scaler = GradScaler() if config.use_amp else None
        self.amp_dtype = torch.float16 if config.amp_dtype == "float16" else torch.bfloat16
        
        # Tracking
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        
        # Logging
        self._setup_logging()
        
        # Resume from checkpoint if specified
        if config.resume_from:
            self.load_checkpoint(config.resume_from)
    
    def _setup_distributed(self):
        """Initialize distributed training"""
        if not dist.is_initialized():
            dist.init_process_group(backend='nccl')
        self.config.local_rank = dist.get_rank()
        self.config.world_size = dist.get_world_size()
        torch.cuda.set_device(self.config.local_rank)
    
    def _get_dataloader(self, dataset: Dataset, shuffle: bool) -> DataLoader:
        """Create dataloader with proper settings"""
        sampler = None
        if self.is_distributed:
            sampler = DistributedSampler(dataset, shuffle=shuffle)
            shuffle = False  # Sampler handles shuffling
        
        return DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            prefetch_factor=self.config.prefetch_factor,
            persistent_workers=True if self.config.num_workers > 0 else False
        )
    
    def _setup_logging(self):
        """Setup logging backends"""
        # Python logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # WandB
        if self.config.use_wandb and (not self.is_distributed or self.config.local_rank == 0):
            run_name = self.config.run_name or f"ghostbot_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            wandb.init(
                project=self.config.project_name,
                name=run_name,
                config=self.config.__dict__
            )
    
    def train_epoch(self) -> Dict[str, float]:
        """Train for one epoch"""
        self.model.train()
        epoch_losses = []
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch}")
        for batch_idx, batch in enumerate(pbar):
            # Move to device
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # Forward pass with mixed precision
            with autocast(enabled=self.config.use_amp, dtype=self.amp_dtype):
                outputs = self.model(
                    visual=batch.get('visual'),
                    audio=batch.get('audio'),
                    language=batch.get('language'),
                    touch=batch.get('touch'),
                    proprio=batch.get('proprio'),
                    vestib=batch.get('vestib'),
                    bci_signal=batch.get('bci')
                )
                
                loss, loss_dict = self.criterion(outputs, batch)
                loss = loss / self.config.gradient_accumulation_steps
            
            # Backward pass
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Optimizer step (with gradient accumulation)
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 
                                                   self.config.max_grad_norm)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 
                                                   self.config.max_grad_norm)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
                self.scheduler.step()
                self.global_step += 1
                
                # Logging
                if self.global_step % self.config.log_every == 0:
                    self._log_metrics(loss_dict, prefix="train")
                
                # Validation
                if self.global_step % self.config.eval_every == 0:
                    val_metrics = self.validate()
                    self._log_metrics(val_metrics, prefix="val")
                
                # Checkpointing
                if self.global_step % self.config.save_every == 0:
                    self.save_checkpoint()
            
            epoch_losses.append(loss_dict['total'])
            pbar.set_postfix({k: f"{v:.4f}" for k, v in loss_dict.items()})
        
        return {'epoch_loss': np.mean(epoch_losses)}
    
    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """Run validation"""
        self.model.eval()
        val_losses = []
        
        for batch in self.val_loader:
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            outputs = self.model(
                visual=batch.get('visual'),
                audio=batch.get('audio'),
                language=batch.get('language'),
                touch=batch.get('touch'),
                proprio=batch.get('proprio'),
                vestib=batch.get('vestib'),
                bci_signal=batch.get('bci')
            )
            
            _, loss_dict = self.criterion(outputs, batch)
            val_losses.append(loss_dict['total'])
        
        self.model.train()
        return {'val_loss': np.mean(val_losses)}
    
    def _log_metrics(self, metrics: Dict[str, float], prefix: str = ""):
        """Log metrics to all backends"""
        if prefix:
            metrics = {f"{prefix}/{k}": v for k, v in metrics.items()}
        
        # Console
        self.logger.info(f"Step {self.global_step}: {metrics}")
        
        # WandB
        if self.config.use_wandb and (not self.is_distributed or self.config.local_rank == 0):
            metrics['learning_rate'] = self.scheduler.get_last_lr()[0]
            wandb.log(metrics, step=self.global_step)
    
    def save_checkpoint(self, is_best: bool = False):
        """Save training checkpoint"""
        if self.is_distributed and self.config.local_rank != 0:
            return
        
        checkpoint_dir = Path(self.config.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'global_step': self.global_step,
            'epoch': self.current_epoch,
            'model_state_dict': self.model.module.state_dict() if self.is_distributed else self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.base_scheduler.state_dict() if self.scheduler.base_scheduler else None,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config.__dict__
        }
        
        # Save latest
        torch.save(checkpoint, checkpoint_dir / 'latest.pt')
        
        # Save numbered checkpoint
        torch.save(checkpoint, checkpoint_dir / f'checkpoint_{self.global_step}.pt')
        
        # Save best
        if is_best:
            torch.save(checkpoint, checkpoint_dir / 'best.pt')
        
        # Cleanup old checkpoints
        self._cleanup_checkpoints()
        
        self.logger.info(f"Saved checkpoint at step {self.global_step}")
    
    def _cleanup_checkpoints(self):
        """Remove old checkpoints, keeping only last N"""
        checkpoint_dir = Path(self.config.checkpoint_dir)
        checkpoints = sorted(checkpoint_dir.glob('checkpoint_*.pt'))
        
        if len(checkpoints) > self.config.keep_last_n:
            for ckpt in checkpoints[:-self.config.keep_last_n]:
                ckpt.unlink()
    
    def load_checkpoint(self, path: str):
        """Load checkpoint"""
        self.logger.info(f"Loading checkpoint from {path}")
        checkpoint = torch.load(path, map_location=self.device)
        
        # Load model
        if self.is_distributed:
            self.model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # Load optimizer & scheduler
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if checkpoint['scheduler_state_dict'] and self.scheduler.base_scheduler:
            self.scheduler.base_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        if checkpoint['scaler_state_dict'] and self.scaler:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        # Load tracking
        self.global_step = checkpoint['global_step']
        self.current_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint['best_val_loss']
        
        self.logger.info(f"Resumed from step {self.global_step}, epoch {self.current_epoch}")
    
    def train(self):
        """Main training loop"""
        self.logger.info("Starting training...")
        self.logger.info(f"Total epochs: {self.config.num_epochs}")
        self.logger.info(f"Steps per epoch: {len(self.train_loader)}")
        self.logger.info(f"Total steps: {len(self.train_loader) * self.config.num_epochs}")
        
        for epoch in range(self.current_epoch, self.config.num_epochs):
            self.current_epoch = epoch
            
            # Set epoch for distributed sampler
            if self.is_distributed and hasattr(self.train_loader.sampler, 'set_epoch'):
                self.train_loader.sampler.set_epoch(epoch)
            
            # Train epoch
            epoch_metrics = self.train_epoch()
            self.logger.info(f"Epoch {epoch} complete: {epoch_metrics}")
            
            # End of epoch validation
            val_metrics = self.validate()
            self._log_metrics(val_metrics, prefix="val")
            
            # Save best model
            if val_metrics['val_loss'] < self.best_val_loss:
                self.best_val_loss = val_metrics['val_loss']
                self.save_checkpoint(is_best=True)
            
            # Regular checkpoint
            self.save_checkpoint()
        
        self.logger.info("Training complete!")
        if self.config.use_wandb:
            wandb.finish()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Main training script"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train Ghost Bot BCI')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    args = parser.parse_args()
    
    # Load config
    if os.path.exists(args.config):
        config = TrainingConfig.from_yaml(args.config)
    else:
        print(f"Config file {args.config} not found, using defaults")
        config = TrainingConfig()
    
    if args.resume:
        config.resume_from = args.resume
    
    # Set seed for reproducibility
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    
    # Import and create model
    # NOTE: Uncomment when ghost_bot_bci.py is in the same directory
    # from ghost_bot_bci import GhostBotBCI
    # model = GhostBotBCI(
    #     vocab_size=config.vocab_size,
    #     embed_dim=config.embed_dim,
    #     num_layers=config.num_layers,
    #     num_affect=config.num_affect,
    #     num_heads=config.num_heads,
    #     mem_size=config.mem_size,
    #     stream_len=config.stream_len,
    #     n_mels=config.n_mels,
    #     img_size=config.img_size,
    #     num_joints=config.num_joints,
    #     bci_channels=config.bci_channels
    # )
    
    # For demonstration, create a dummy model
    print("="*70)
    print("GHOST BOT BCI TRAINER - Professional Grade")
    print("="*70)
    print("\nIMPORTANT: Uncomment the model import in main() to use real Ghost Bot")
    print("Currently using dummy model for demonstration\n")
    
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.dummy = nn.Linear(10, 10)
        def forward(self, **kwargs):
            B = kwargs.get('visual', torch.zeros(1, 1, 3, 224, 224)).size(0)
            return {
                'language_logits': torch.randn(B, 30, config.vocab_size),
                'collision_data': torch.randn(B, 30, config.embed_dim),
                'coherence': torch.tensor(0.8),
                'emotion': torch.randn(B, config.num_affect)
            }
    
    model = DummyModel()
    
    # Print config
    print(f"Configuration:")
    print(f"  Batch size: {config.batch_size}")
    print(f"  Learning rate: {config.learning_rate}")
    print(f"  Epochs: {config.num_epochs}")
    print(f"  Mixed precision: {config.use_amp} ({config.amp_dtype})")
    print(f"  Distributed: {config.use_ddp or config.use_fsdp}")
    print(f"  Device: {config.device}")
    print(f"  Gradient accumulation: {config.gradient_accumulation_steps}")
    print(f"  Max grad norm: {config.max_grad_norm}")
    print(f"  Optimizer: {config.optimizer}")
    print(f"  Scheduler: {config.scheduler} (warmup: {config.warmup_steps})")
    print(f"  Checkpoint dir: {config.checkpoint_dir}")
    print(f"  WandB: {config.use_wandb}")
    print()
    
    # Create trainer
    trainer = GhostBotTrainer(model, config)
    
    # Train
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nTraining interrupted by user")
        trainer.save_checkpoint()
        print("Checkpoint saved. Resume with --resume checkpoints/latest.pt")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXAMPLE CONFIG FILE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXAMPLE_CONFIG_YAML = """
# Ghost Bot BCI Training Configuration

# Model Architecture
vocab_size: 10000
embed_dim: 256
num_layers: 6
num_affect: 8
num_heads: 8
mem_size: 150
stream_len: 24
n_mels: 80
img_size: 224
num_joints: 24
bci_channels: 64

# Training Hyperparameters
batch_size: 8
num_epochs: 100
learning_rate: 0.0003
weight_decay: 0.01
warmup_steps: 1000
max_grad_norm: 1.0
gradient_accumulation_steps: 4

# Optimization
optimizer: adamw  # adamw, adam, sgd
scheduler: cosine  # cosine, linear, constant
betas: [0.9, 0.999]
eps: 1.0e-08

# Mixed Precision
use_amp: true
amp_dtype: float16  # float16 or bfloat16

# Distributed Training
use_ddp: false
use_fsdp: false

# Data
data_path: ./data
num_workers: 4
pin_memory: true
prefetch_factor: 2

# Checkpointing
checkpoint_dir: ./checkpoints
save_every: 1000
keep_last_n: 5
resume_from: null

# Logging
log_every: 10
eval_every: 500
use_wandb: true
use_tensorboard: false
project_name: ghostbot-bci
run_name: null

# Loss Weights
loss_weights:
  language: 1.0
  collision: 0.5
  coherence: 0.3
  emotion: 0.2
  reconstruction: 0.1

# Hardware
device: cuda
seed: 42
"""


def create_example_config():
    """Create an example config.yaml file"""
    if not os.path.exists('config.yaml'):
        with open('config.yaml', 'w') as f:
            f.write(EXAMPLE_CONFIG_YAML)
        print("Created example config.yaml")
    else:
        print("config.yaml already exists, skipping")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA PREPARATION UTILITIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_dummy_dataset(output_dir: str = "./data", num_train: int = 100, num_val: int = 20):
    """Create dummy dataset for testing"""
    print(f"Creating dummy dataset with {num_train} train and {num_val} val samples...")
    
    for split, num_samples in [('train', num_train), ('val', num_val)]:
        split_dir = Path(output_dir) / split
        split_dir.mkdir(parents=True, exist_ok=True)
        
        for i in range(num_samples):
            seq_dir = split_dir / f"seq_{i:05d}"
            seq_dir.mkdir(exist_ok=True)
            
            # Create dummy data for each modality
            np.save(seq_dir / 'visual.npy', np.random.randn(30, 3, 224, 224).astype(np.float32))
            np.save(seq_dir / 'audio.npy', np.random.randn(30, 80).astype(np.float32))
            np.save(seq_dir / 'language.npy', np.random.randint(0, 10000, (30,)).astype(np.int64))
            np.save(seq_dir / 'touch.npy', np.random.randn(30, 1, 32, 32).astype(np.float32))
            np.save(seq_dir / 'proprio.npy', np.random.randn(30, 24).astype(np.float32))
            np.save(seq_dir / 'vestib.npy', np.random.randn(30, 6).astype(np.float32))
            np.save(seq_dir / 'bci.npy', np.random.randn(30, 64, 250).astype(np.float32))
            np.save(seq_dir / 'labels.npy', np.random.randint(0, 10000, (30,)).astype(np.int64))
    
    print(f"Dummy dataset created at {output_dir}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DISTRIBUTED TRAINING LAUNCHER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def launch_distributed():
    """Launch distributed training with torchrun"""
    import subprocess
    import sys
    
    print("Launching distributed training...")
    print("Usage: torchrun --nproc_per_node=NUM_GPUS train.py --config config.yaml")
    print()
    print("Example for 4 GPUs:")
    print("  torchrun --nproc_per_node=4 train.py --config config.yaml")
    print()
    print("For multi-node:")
    print("  torchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \\")
    print("           --master_addr=MASTER_IP --master_port=29500 \\")
    print("           train.py --config config.yaml")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI COMMANDS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'create-config':
        create_example_config()
    elif len(sys.argv) > 1 and sys.argv[1] == 'create-dummy-data':
        create_dummy_dataset()
    elif len(sys.argv) > 1 and sys.argv[1] == 'distributed-help':
        launch_distributed()
    else:
        main()


"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USAGE EXAMPLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Create example config
python train.py create-config

# 2. Create dummy dataset for testing
python train.py create-dummy-data

# 3. Train on single GPU
python train.py --config config.yaml

# 4. Resume from checkpoint
python train.py --config config.yaml --resume checkpoints/latest.pt

# 5. Distributed training on 4 GPUs
torchrun --nproc_per_node=4 train.py --config config.yaml

# 6. Multi-node distributed (4 nodes, 8 GPUs each)
# On master node (rank 0):
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \
         --master_addr=192.168.1.100 --master_port=29500 \
         train.py --config config.yaml

# On worker nodes (rank 1, 2, 3):
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=RANK \
         --master_addr=192.168.1.100 --master_port=29500 \
         train.py --config config.yaml

# 7. View distributed training help
python train.py distributed-help

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FEATURES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Distributed Data Parallel (DDP) support
âœ“ Fully Sharded Data Parallel (FSDP) ready
âœ“ Mixed precision training (FP16/BF16)
âœ“ Gradient accumulation
âœ“ Gradient clipping
âœ“ Learning rate warmup
âœ“ Multiple optimizers (AdamW, Adam, SGD)
âœ“ Multiple schedulers (Cosine, Linear, Constant)
âœ“ Automatic checkpointing
âœ“ Resume from checkpoint
âœ“ WandB & TensorBoard logging
âœ“ Modular dataset pipeline
âœ“ Multi-task loss
âœ“ Progress bars
âœ“ Distributed sampler
âœ“ Prefetching & pin memory
âœ“ Professional logging
âœ“ Config file support (YAML)
âœ“ Curriculum learning ready
âœ“ Production-ready error handling

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NEXT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Save ghost_bot_bci.py and train.py in same directory
2. Run: python train.py create-config
3. Edit config.yaml to your needs
4. Run: python train.py create-dummy-data (for testing)
5. Uncomment the real model import in main()
6. Run: python train.py --config config.yaml
7. Monitor with WandB dashboard
8. Scale to multiple GPUs with torchrun

For photonic collision experiments:
- Train two separate models with different human BCI data
- Export collision data from both using export_collision_jsonl()
- Convert .jsonl â†’ binary â†’ photonic signal
- COLLIDE at light speed! ğŸ’¥

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
